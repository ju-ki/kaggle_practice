{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import typing as tp\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cat\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predicted, actual):\n",
    "    return np.sqrt(((predicted - actual) ** 2).mean())\n",
    "\n",
    "def split_cat(text):\n",
    "    try:\n",
    "        return text.split(\"/\")\n",
    "    except:\n",
    "        return (\"No label\", \"No label\", \"No label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetEncoder:\n",
    "    def __repr__(self):\n",
    "        return \"TargetEnconer\"\n",
    "    def __init__(self, cols, smoothing=1, min_samples_leaf=1, noise_level=0, keep_original=False):\n",
    "        self.cols = cols\n",
    "        self.smoothing = smoothing\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.noise_level = noise_level\n",
    "        self.keep_original = keep_original\n",
    "        \n",
    "    @staticmethod\n",
    "    def add_nosie(series, noise_level):\n",
    "        return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "    \n",
    "    def encode(self, train, test, target):\n",
    "        for col in self.cols:\n",
    "            if self.keep_original:\n",
    "                train[col + \"_te\"], test[col + \"_te\"] = self.encode_column(train[col], test[col], target)\n",
    "            else:\n",
    "                train[col], test[col] = self.encode_column(train[col], test[col], target)\n",
    "        return train, test\n",
    "    \n",
    "    def encode_column(self, train_series, test_series, target):\n",
    "        temp = pd.concat([train_series, test_series], axis=1)\n",
    "        \n",
    "        averages = temp.groupby(by=train_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "        \n",
    "        smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - self.min_samples_leaf) / self.smoothing))\n",
    "        \n",
    "        prior = target.mean()\n",
    "        \n",
    "        averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "        averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "        \n",
    "        ft_train_series = pd.merge(\n",
    "           train_series.to_frame(train_series.name),\n",
    "           averages.reset_idex().rename(columns={\"index\":target.name, target.name:\"average\"}),\n",
    "           on = train_series.name, how=\"left\")[\"average\"].rename(train_series.name + \"_mean\").fillna(prior)\n",
    "        \n",
    "        ft_train_series.index = train_series.index\n",
    "        \n",
    "        ft_test_series = pd.merge(\n",
    "           test_series.to_frame(test_series.name),\n",
    "           averages.reset_idex().rename(columns={\"index\":target.name, target.name:\"average\"}),\n",
    "           on = test_series.name, how=\"left\")[\"average\"].rename(train_series.name + \"_mean\").fillna(prior)\n",
    "\n",
    "        ft_test_series.index = test_series.index\n",
    "        \n",
    "        return self.add_nosie(ft_train_series, self.noise_level), self.add_nosie(ft_test_series, self.noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_number(x):\n",
    "    try:\n",
    "        if not x.isdigit():\n",
    "            return 0\n",
    "        x = int(x)\n",
    "        if x > 100:\n",
    "            return 100\n",
    "        else:\n",
    "            return x\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def sum_number(desc):\n",
    "    if not isinstance(desc, str):\n",
    "        return 0\n",
    "    try:\n",
    "        return sum([to_number(s) for s in desc.split()])\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "stopwords = {x: 1 for x in stopwords.words(\"english\")}\n",
    "non_alphanums = re.compile(u'[^A-za-z0-9]+')\n",
    "non_alphanumpunct = re.compile(u'[^A-Za-z0-9\\.?!, ; \\(\\)\\[\\]\\'\\\"\\$]+')\n",
    "RE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])\n",
    "\n",
    "def normalize_text(text):\n",
    "    return u\" \".join(\n",
    "    [x for x in [y for y in non_alphanums.sub(' ', text).lower().strip().split(\" \")]\\\n",
    "    if len(x) > 1 and x not in stopwords])\n",
    "\n",
    "def clean_name(x):\n",
    "    if len(x):\n",
    "        x = non_alphanums.sub(' ', x).split()\n",
    "        if len(x):\n",
    "            return x[0].lower()\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0748295783996582\n"
     ]
    }
   ],
   "source": [
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished load data:35.7831609249115\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/work/kaggle_practice/Mercari/input/\")\n",
    "train = pd.read_table(\"train.tsv\", engine=\"c\", dtype={\"item_condition_id\":\"category\",\n",
    "                                                     \"shipping\":\"category\"},\n",
    "                     converters={\"category_name\":split_cat})\n",
    "\n",
    "test = pd.read_table(\"test.tsv\", engine=\"c\", dtype={\"item_condition_id\":\"category\",\n",
    "                                                     \"shipping\":\"category\"},\n",
    "                     converters={\"category_name\":split_cat})\n",
    "print(\"Finished load data:{}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled train / test:37.371657848358154\n",
      "Train shape: (1482535, 9) Test shape: (693359, 8)\n"
     ]
    }
   ],
   "source": [
    "train[\"is_train\"] = 1\n",
    "test[\"is_train\"] = 0\n",
    "print(\"Compiled train / test:{}\".format(time.time() - start_time))\n",
    "print(\"Train shape:\", train.shape, \"Test shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed nonzero price:41.97385311126709\n",
      "Train shape: (1481661, 9) Test shape: (693359, 8)\n"
     ]
    }
   ],
   "source": [
    "train = train[train.price != 0].reset_index(drop=True)\n",
    "print(\"Removed nonzero price:{}\".format(time.time() - start_time))\n",
    "print(\"Train shape:\", train.shape, \"Test shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled merge:45.75414729118347\n",
      "Merge shape: (2175020, 10)\n"
     ]
    }
   ],
   "source": [
    "y = np.log1p(train[\"price\"])\n",
    "nrow_train = train.shape[0]\n",
    "\n",
    "merge = pd.concat([train, test])\n",
    "submission = test[[\"test_id\"]]\n",
    "print(\"Compiled merge:{}\".format(time.time() - start_time))\n",
    "print(\"Merge shape:\", merge.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garbage collection:52.63427662849426\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "del train \n",
    "del test\n",
    "merge.drop([\"train_id\", \"test_id\", \"price\"], axis=1, inplace=True)\n",
    "gc.collect()\n",
    "print(\"Garbage collection:{}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split categories completed:62.509881258010864\n"
     ]
    }
   ],
   "source": [
    "merge[\"gencat_name\"] = merge[\"category_name\"].str.get(0).replace(\"\", \"missing\").astype(\"category\")\n",
    "merge[\"subcat1_name\"] = merge[\"category_name\"].str.get(1).fillna(\"missing\").astype(\"category\")\n",
    "merge[\"subcat2_name\"] = merge[\"category_name\"].str.get(2).fillna(\"missing\").astype(\"category\")\n",
    "merge.drop(\"category_name\", axis=1, inplace=True)\n",
    "print(\"Split categories completed:{}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handle missing completed.:77.75830817222595\n"
     ]
    }
   ],
   "source": [
    "merge[\"item_condition_id\"] = merge[\"item_condition_id\"].cat.add_categories([\"missing\"]).fillna(\"missing\")\n",
    "merge[\"shipping\"] = merge[\"shipping\"].cat.add_categories([\"missing\"]).fillna(\"missing\")\n",
    "merge[\"item_description\"].fillna(\"missing\", inplace=True)\n",
    "merge[\"brand_name\"] = merge[\"brand_name\"].fillna(\"missing\").astype(\"category\")\n",
    "print(\"Handle missing completed.:{}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE 1/37:88.37726712226868\n"
     ]
    }
   ],
   "source": [
    "merge[\"name_first\"] = merge[\"name\"].apply(clean_name)\n",
    "print(\"FE 1/37:{}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE 2/37:90.57625770568848\n"
     ]
    }
   ],
   "source": [
    "merge[\"name_first_count\"] = merge.groupby(\"name_first\")[\"name_first\"].transform(\"count\")\n",
    "print(\"FE 2/37:{}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE 3/37:92.3014121055603\n",
      "FE 4/37:92.43728756904602\n",
      "FE 5/37:92.47120976448059\n",
      "FE 6/37:92.49770474433899\n"
     ]
    }
   ],
   "source": [
    "merge[\"gencat_name_count\"] = merge.groupby(\"gencat_name\")[\"gencat_name\"].transform(\"count\")\n",
    "print(\"FE 3/37:{}\".format(time.time() - start_time))\n",
    "merge[\"subcat1_name_count\"] = merge.groupby(\"subcat1_name\")[\"subcat1_name\"].transform(\"count\")\n",
    "print(\"FE 4/37:{}\".format(time.time() - start_time))\n",
    "merge[\"subcat2_name_count\"] = merge.groupby(\"subcat2_name\")[\"subcat2_name\"].transform(\"count\")\n",
    "print(\"FE 5/37:{}\".format(time.time() - start_time))\n",
    "merge[\"brand_name_count\"] = merge.groupby(\"brand_name\")[\"brand_name\"].transform(\"count\")\n",
    "print(\"FE 6/37:{}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE 7/37:106.43829584121704\n",
      "FE 8/37:165.35912609100342\n",
      "FE 9/37:171.03732633590698\n",
      "FE 10/37:187.1764430999756\n"
     ]
    }
   ],
   "source": [
    "merge[\"NameLower\"] = merge.name.str.count(\"[a-z]\")\n",
    "print(\"FE 7/37:{}\".format(time.time() - start_time))\n",
    "merge[\"DescriptionLower\"] = merge.item_description.str.count(\"[a-z]\")\n",
    "print(\"FE 8/37:{}\".format(time.time() - start_time))\n",
    "merge[\"NameUpper\"] = merge.name.str.count(\"[A-Z]\")\n",
    "print(\"FE 9/37:{}\".format(time.time() - start_time))\n",
    "merge[\"DescriptionUpper\"] = merge.item_description.str.count(\"[A-Z]\")\n",
    "print(\"FE 10/37:{}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE 11/37:192.06892848014832\n",
      "FE 12/37:193.76055669784546\n",
      "FE 13/37:193.78000116348267\n",
      "FE 14/37:199.76943516731262\n",
      "FE 15/37:206.85459876060486\n",
      "FE 16/37:209.19546604156494\n",
      "FE 17/37:212.16957783699036\n",
      "FE 18/37:212.28700733184814\n",
      "FE 19/37:212.2971212863922\n",
      "FE 20/37:212.30767107009888\n",
      "FE 21/37:212.33090829849243\n",
      "FE 22/37:212.3795886039734\n",
      "FE 23/37:212.48866724967957\n"
     ]
    }
   ],
   "source": [
    "merge[\"name_len\"] = merge[\"name\"].apply(lambda x:len(x))\n",
    "print(\"FE 11/37:{}\".format(time.time() - start_time))\n",
    "merge[\"des_len\"] = merge[\"item_description\"].apply(lambda x:len(x))\n",
    "print(\"FE 12/37:{}\".format(time.time() - start_time))\n",
    "merge[\"name_desc_len_ratio\"] = merge[\"name_len\"] - merge[\"des_len\"]\n",
    "print(\"FE 13/37:{}\".format(time.time() - start_time))\n",
    "merge[\"desc_word_count\"] = merge[\"item_description\"].apply(lambda x: len(x.split()))\n",
    "print(\"FE 14/37:{}\".format(time.time() - start_time))\n",
    "merge[\"mean_des\"] = merge[\"item_description\"].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) / len(x) * 10)\n",
    "print(\"FE 15/37:{}\".format(time.time() - start_time))\n",
    "merge[\"name_word_count\"] = merge[\"name\"].apply(lambda x: len(x.split()))\n",
    "print(\"FE 16/37:{}\".format(time.time() - start_time))\n",
    "merge[\"mean_name\"] = merge[\"name\"].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) / len(x) * 10)\n",
    "print(\"FE 17/37:{}\".format(time.time() - start_time))\n",
    "merge[\"desc_letters_per_word\"] = merge[\"des_len\"] / merge[\"desc_word_count\"]\n",
    "print(\"FE 18/37:{}\".format(time.time() - start_time))\n",
    "merge[\"name_letters_per_word\"] = merge[\"name_len\"] / merge[\"name_word_count\"]\n",
    "print(\"FE 19/37:{}\".format(time.time() - start_time))\n",
    "merge[\"NameLowerRatio\"] = merge[\"NameLower\"] / merge[\"name_len\"]\n",
    "print(\"FE 20/37:{}\".format(time.time() - start_time))\n",
    "merge[\"DescriptionLowerRatio\"] = merge[\"DescriptionLower\"] / merge[\"des_len\"]\n",
    "print(\"FE 21/37:{}\".format(time.time() - start_time))\n",
    "merge[\"NameUpperRatio\"] = merge[\"NameUpper\"] / merge[\"name_len\"]\n",
    "print(\"FE 22/37:{}\".format(time.time() - start_time))\n",
    "merge[\"DescriptionUpperRatio\"] = merge[\"DescriptionUpper\"] / merge[\"des_len\"]\n",
    "print(\"FE 23/37:{}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE 24/37:222.44420886039734\n",
      "FE 25/37:234.08341526985168\n",
      "FE 26/37:234.12060236930847\n",
      "FE 27/37:234.13272047042847\n"
     ]
    }
   ],
   "source": [
    "merge[\"NamePunctCount\"] = merge.name.str.count(RE_PUNCTUATION)\n",
    "print(\"FE 24/37:{}\".format(time.time() - start_time))\n",
    "merge[\"DescriptionPunctCount\"] = merge.item_description.str.count(RE_PUNCTUATION)\n",
    "print(\"FE 25/37:{}\".format(time.time() - start_time))\n",
    "merge[\"NamePunctCountRatio\"] = merge[\"NamePunctCount\"] / merge[\"name_word_count\"]\n",
    "print(\"FE 26/37:{}\".format(time.time() - start_time))\n",
    "merge[\"DescriptionPunctCountRatio\"] = merge[\"DescriptionPunctCount\"] / merge[\"desc_word_count\"]\n",
    "print(\"FE 27/37:{}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE 28/37:244.15404868125916\n",
      "FE 29/37:253.99641227722168\n",
      "FE 30/37:254.00394558906555\n",
      "FE 31/37:254.01152348518372\n"
     ]
    }
   ],
   "source": [
    "merge[\"NameDigitCount\"] = merge.name.str.count(\"[0-9]\")\n",
    "print(\"FE 28/37:{}\".format(time.time() - start_time))\n",
    "merge[\"DescriptionDigitCount\"] = merge.item_description.str.count(\"[0-9]\")\n",
    "print(\"FE 29/37:{}\".format(time.time() - start_time))\n",
    "merge[\"NameDigitCountRatio\"] = merge[\"NameDigitCount\"] / merge[\"name_word_count\"]\n",
    "print(\"FE 30/37:{}\".format(time.time() - start_time))\n",
    "merge[\"DescriptionDigitCountRatio\"] = merge[\"DescriptionDigitCount\"] / merge[\"desc_word_count\"]\n",
    "print(\"FE 31/37:{}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE 32/37:269.2812809944153\n",
      "FE 33/37:288.58040499687195\n",
      "FE 34/37:301.6157257556915\n",
      "FE 35/37:305.76182794570923\n",
      "FE 36/37:319.7226526737213\n",
      "FE 37/37:320.6909189224243\n"
     ]
    }
   ],
   "source": [
    "merge[\"stopword_ratio_desc\"] = merge[\"item_description\"].apply(lambda x: len([w for w in x.split() if w in stopwords])) / merge[\"desc_word_count\"]\n",
    "print(\"FE 32/37:{}\".format(time.time() - start_time))\n",
    "merge[\"num_sum\"] = merge[\"item_description\"].apply(sum_number)\n",
    "print(\"FE 33/37:{}\".format(time.time() - start_time))\n",
    "merge[\"weird_characters_desc\"] = merge[\"item_description\"].str.count(non_alphanumpunct)\n",
    "print(\"FE 34/37:{}\".format(time.time() - start_time))\n",
    "merge[\"weird_characters_name\"] = merge[\"name\"].str.count(non_alphanumpunct)\n",
    "print(\"FE 35/37:{}\".format(time.time() - start_time))\n",
    "merge[\"prices_count\"] = merge[\"item_description\"].str.count(\"[rm]\")\n",
    "print(\"FE 36/37:{}\".format(time.time() - start_time))\n",
    "merge[\"prices_in_name\"] = merge[\"item_description\"].str.contains(\"[rm]\", regex=False).astype(\"int\")\n",
    "print(\"FE 37/37:{}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE Normalized:444.6609630584717\n"
     ]
    }
   ],
   "source": [
    "cols = set(merge.columns.values)\n",
    "basic_cols = {\"name\", \"item_condition_id\", \"brand_name\", \"shipping\", \"item_description\", \"gencat_name\",\n",
    "             \"subcat1_name\", \"subcat2_name\", \"name_first\", \"is_train\"}\n",
    "\n",
    "cols_to_normalize = cols - basic_cols - {\"prices_in_name\"}\n",
    "other_cols = basic_cols | {\"prices_in_name\"}\n",
    "\n",
    "merge_to_normalize = merge[list(cols_to_normalize)]\n",
    "merge_to_normalize = (merge_to_normalize - merge_to_normalize.mean()) / (merge_to_normalize.max() - merge_to_normalize.min())\n",
    "\n",
    "print(\"FE Normalized:{}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE Merged:481.3497984409332\n"
     ]
    }
   ],
   "source": [
    "merge = merge[list(other_cols)]\n",
    "merge = pd.concat([merge, merge_to_normalize], axis=1)\n",
    "\n",
    "print(\"FE Merged:{}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garbage collection:552.5701448917389\n"
     ]
    }
   ],
   "source": [
    "del (merge_to_normalize)\n",
    "gc.collect()\n",
    "print(\"Garbage collection:{}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splittin completed:881.2715878486633\n"
     ]
    }
   ],
   "source": [
    "SUBMIT_MODE = True\n",
    "df_test = merge.loc[merge[\"is_train\"] == 0]\n",
    "df_train = merge.loc[merge[\"is_train\"] == 1]\n",
    "del merge\n",
    "gc.collect()\n",
    "df_test = df_test.drop([\"is_train\"], axis=1)\n",
    "df_train = df_train.drop([\"is_train\"], axis=1)\n",
    "\n",
    "if SUBMIT_MODE:\n",
    "    y_train = y\n",
    "    del y\n",
    "    gc.collect()\n",
    "else:\n",
    "    df_train, df_test, y_train, y_test = train_test_split(df_train, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Splittin completed:{}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
